# -*- coding: utf-8 -*-
"""Capstone 1_ML_ZoomCamp_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onSiXz2afhOsuTj--gqQwjXECeMTmMF7

### CAPSTONE PROJECT 1

### LIBRARIES :
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from prettytable import PrettyTable
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import RFE
from sklearn.base import clone
from sklearn.metrics import r2_score
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import EarlyStopping
import keras_tuner as kt
from sklearn.inspection import permutation_importance

"""### READ THE DATASET :"""

df = pd.read_csv('space_missions_dataset.csv')
df.head()

df.tail()

"""### TRAIN/VALIDATION/TEST SPLIT
"""

df_temp, df_test = train_test_split(df, test_size=0.2, random_state=1)
df_train, df_val = train_test_split(df_temp, test_size=0.25, random_state=1)

df_train.reset_index(drop=True, inplace=True)
df_val.reset_index(drop=True, inplace=True)
df_test.reset_index(drop=True, inplace=True)

target_train = df_train['Mission Success (%)'].values
target_validation = df_val['Mission Success (%)'].values
target_test = df_test['Mission Success (%)'].values

df_train.drop(columns=['Mission Success (%)'], inplace=True)
df_val.drop(columns=['Mission Success (%)'], inplace=True)
df_test.drop(columns=['Mission Success (%)'], inplace=True)

print(f"Training set shape: {df_train.shape}, Target shape: {target_train.shape}")
print(f"Validation set shape: {df_val.shape}, Target shape: {target_validation.shape}")
print(f"Test set shape: {df_test.shape}, Target shape: {target_test.shape}")

"""### DICTVECTORIZER"""

vectorizer = DictVectorizer(sparse=True)

train_records = df_train.to_dict(orient='records')
X_train_vectorized = vectorizer.fit_transform(train_records)

validation_records = df_val.to_dict(orient='records')
X_validation_vectorized = vectorizer.transform(validation_records)

test_records = df_test.to_dict(orient='records')
X_test_vectorized = vectorizer.transform(test_records)

print(f"Shape of vectorized training data: {X_train_vectorized.shape}")
print(f"Shape of vectorized validation data: {X_validation_vectorized.shape}")
print(f"Shape of vectorized test data: {X_test_vectorized.shape}")

"""### FUNCTION FOR EVALUATION METRICS"""

def evaluate_model(model, X_train_vectorized, target_train,
                   X_validation_vectorized, target_validation,
                   X_test_vectorized, target_test):

    train_predictions = model.predict(X_train_vectorized)
    val_predictions = model.predict(X_validation_vectorized)
    test_predictions = model.predict(X_test_vectorized)

    print("Training Set Evaluation:")
    rmse_train = np.sqrt(mean_squared_error(target_train, train_predictions))
    mae_train = mean_absolute_error(target_train, train_predictions)
    r2_train = r2_score(target_train, train_predictions)
    print(f'  RMSE: {rmse_train:.3f}')
    print(f'  MAE: {mae_train:.3f}')
    print(f'  R²: {r2_train:.3f}\n')

    print("Validation Set Evaluation:")
    rmse_val = np.sqrt(mean_squared_error(target_validation, val_predictions))
    mae_val = mean_absolute_error(target_validation, val_predictions)
    r2_val = r2_score(target_validation, val_predictions)
    print(f'  RMSE: {rmse_val:.3f}')
    print(f'  MAE: {mae_val:.3f}')
    print(f'  R²: {r2_val:.3f}\n')

    print("Test Set Evaluation:")
    rmse_test = np.sqrt(mean_squared_error(target_test, test_predictions))
    mae_test = mean_absolute_error(target_test, test_predictions)
    r2_test = r2_score(target_test, test_predictions)
    print(f'  RMSE: {rmse_test:.3f}')
    print(f'  MAE: {mae_test:.3f}')
    print(f'  R²: {r2_test:.3f}\n')

"""### DIMENSIONALITY REDUCTION AND MODEL TRAINING WITHOUT PARAMETER TUNING (PCA)

## Linear regression
"""

pca = PCA(n_components=10)
X_train_pca = pca.fit_transform(X_train_vectorized.toarray())
X_validation_pca = pca.transform(X_validation_vectorized.toarray())
X_test_pca = pca.transform(X_test_vectorized.toarray())

linear_model = LinearRegression()
linear_model.fit(X_train_pca, target_train)

print("Linear Regression with PCA Evaluation:")
evaluate_model(
    linear_model,
    X_train_pca, target_train,
    X_validation_pca, target_validation,
    X_test_pca, target_test
)

"""### CONCLUSION
The linear regression model with PCA demonstrates consistent performance across training, validation, and test sets. The R² values (~0.70-0.75) indicate that the model explains a significant portion of the variance, with moderate prediction errors (RMSE ~4.6-5.0, MAE ~3.8-4.0).
While effective, more advanced models or optimization can improve performance.

### RIDGE REGRESSION WITH GRID SEARCH FOR HYPERPARAMETER TUNING
"""

ridge_param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}
ridge_grid = GridSearchCV(Ridge(), ridge_param_grid, cv=5, scoring='r2')
ridge_grid.fit(X_train_vectorized, target_train)

print(f"Best Ridge Parameters: {ridge_grid.best_params_}")

best_ridge_model = ridge_grid.best_estimator_

print("Best Ridge Regression Model Evaluation:")
evaluate_model(
    best_ridge_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The Ridge Regression model with the best parameter (alpha = 1000.0) demonstrates balanced performance across all datasets. It achieves an R² of 0.706 on the training set, 0.751 on the validation set, and 0.727 on the test set, indicating good generalization with consistent prediction accuracy.
While the model performs well, further optimization or advanced modeling may enhance results.

### OPTIMIZATION OF LASSO REGRESSION HYPERPARAMETERS USING GRIDSEARCHCV
"""

lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}
lasso_grid = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='r2')
lasso_grid.fit(X_train_vectorized, target_train)

print(f"Best Lasso Parameters: {lasso_grid.best_params_}")

best_lasso_model = lasso_grid.best_estimator_

print("Best Lasso Regression Model Evaluation:")
evaluate_model(
    best_lasso_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The Lasso regression model with the optimized alpha value of 10.0 demonstrates balanced performance across training, validation, and test sets. While the training R² indicates a moderate fit (0.701), the validation and test R² values (0.754 and 0.726) highlight the model's ability to generalize well to unseen data.
This suggests that the chosen alpha effectively controls overfitting while maintaining prediction accuracy.

### DECISION TREE REGRESSOR
"""

dt_model = DecisionTreeRegressor(
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
dt_model.fit(X_train_vectorized, target_train)

print("Decision Tree Model Evaluation:")
evaluate_model(
    dt_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The Decision Tree model demonstrates strong performance on the training set with an R² of 0.915, indicating a good fit to the data.
However, the decrease in R² for the validation (0.739) and test (0.774) sets highlights a slight overfitting tendency.
Despite this, the model maintains reasonable generalization capabilities across unseen data, with acceptable RMSE and MAE values.
Further optimization or regularization could improve the balance between training and validation performance.

### XGB REGRESSOR
"""

xgb_model = XGBRegressor(
    n_estimators=20,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42
)
xgb_model.fit(X_train_vectorized, target_train)

print("XGBoost Model Evaluation:")
evaluate_model(
    xgb_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The XGBoost model demonstrates strong predictive performance with an R² of 0.938 on the training set, indicating a well-fitted model.
However, the validation (R² = 0.777) and test (R² = 0.831) results suggest some generalization challenges, likely due to mild overfitting.
Further tuning regularization parameters or adjustments to the training process could enhance model performance on unseen data.

### RANDOM FOREST REGRESSOR
"""

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_vectorized, target_train)

print("Random Forest Model Evaluation:")
evaluate_model(
    rf_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The Random Forest Regressor demonstrates excellent performance with an R² of 0.973 on the training set, indicating a strong fit. The validation and test set results, with R² values of 0.804 and RMSE values of 4.421 and 3.393, respectively, highlight the model's ability to generalize well to unseen data.
This balance suggests that the model effectively captures patterns in the data while avoiding overfitting.

### GRADIENT BOOSTING REGRESSOR
"""

gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train_vectorized, target_train)

print("Gradient Boosting Model Evaluation:")
evaluate_model(
    gb_model,
    X_train_vectorized, target_train,
    X_validation_vectorized, target_validation,
    X_test_vectorized, target_test
)

"""### CONCLUSION
The Gradient Boosting Regressor demonstrates strong performance, achieving high accuracy across all datasets. The model fits the training data well (R²: 0.965) and generalizes effectively to the validation (R²: 0.816) and test sets (R²: 0.863).
This indicates the model captures complex patterns without significant overfitting.

### EVALUATION METRICS: RMSE, MAE, AND R²
"""

model_predictions = {
    "Linear Regression": linear_model.predict(X_validation_pca),
    "Ridge Regression": best_ridge_model.predict(X_validation_vectorized),
    "Lasso Regression": best_lasso_model.predict(X_validation_vectorized),
    "Decision Tree": dt_model.predict(X_validation_vectorized),
    "Random Forest": rf_model.predict(X_validation_vectorized),
    "XGBoost": xgb_model.predict(X_validation_vectorized),
    "Gradient Boosting": gb_model.predict(X_validation_vectorized)
}

evaluation_results = {}
for model_name, predictions in model_predictions.items():
    metrics = {
        "RMSE": np.sqrt(mean_squared_error(target_validation, predictions)),
        "MAE": mean_absolute_error(target_validation, predictions),
        "R2": r2_score(target_validation, predictions)
    }
    evaluation_results[model_name] = metrics

model_metrics = {
    "Model": list(evaluation_results.keys()),
    "RMSE": [metrics["RMSE"] for metrics in evaluation_results.values()],
    "R2 Score": [metrics["R2"] for metrics in evaluation_results.values()],
    "MAE": [metrics["MAE"] for metrics in evaluation_results.values()]
}
metrics_df = pd.DataFrame(model_metrics)

metrics_df

"""### CONCLUSION
The Gradient Boosting model demonstrates the best overall performance among all tested models, achieving the lowest RMSE (4.28) and MAE (2.85) with the highest R² score (0.81).
This indicates that Gradient Boosting provides the most accurate predictions with minimal error and explains the majority of the variance in the target variable.
The Random Forest model also performs well, with an RMSE of 4.42 and an R² of 0.80, making it a strong alternative to Gradient Boosting.
Other models, such as Linear Regression, Ridge Regression, and Lasso Regression, show competitive performance but slightly higher errors,
while the Decision Tree model exhibits lower errors but may overfit the data due to its nature.
The XGBoost model had relatively weaker performance compared to Gradient Boosting and Random Forest, likely due to the specific hyperparameter settings used."""


"""### TRAINING MULTIPLE VARIATIONS OF NEURAL NETWORKS WITH TUNED PARAMETERS"""

input_dim = X_train_vectorized.shape[1]

def build_model(hp):
    model = Sequential()

    model.add(Dense(units=hp.Int('units_layer1', min_value=64, max_value=512, step=64), activation='relu', input_dim=input_dim))
    model.add(BatchNormalization())
    model.add(Dropout(rate=hp.Float('dropout_layer1', min_value=0.0, max_value=0.5, step=0.1)))

    model.add(Dense(units=hp.Int('units_layer2', min_value=32, max_value=256, step=32), activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(rate=hp.Float('dropout_layer2', min_value=0.0, max_value=0.5, step=0.1)))

    if hp.Choice('add_layer3', [True, False]):
        model.add(Dense(units=hp.Int('units_layer3', min_value=32, max_value=128, step=32), activation='relu'))
        model.add(Dropout(rate=hp.Float('dropout_layer3', min_value=0.0, max_value=0.5, step=0.1)))

    model.add(Dense(1))

    model.compile(
        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd']),
        loss='mse',
        metrics=['mae']
    )
    return model

tuner = kt.Hyperband(
    build_model,
    objective='val_mae',
    max_epochs=50,
    factor=2,
    directory='my_tuner_dir',
    project_name='nn_hyperparameter_tuning'
)

early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)

tuner.search(
    X_train_vectorized,
    target_train,
    epochs=5,
    validation_split=0.2,
    callbacks=[early_stopping]
)

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

best_model = tuner.hypermodel.build(best_hps)
history = best_model.fit(
    X_train_vectorized,
    target_train,
    epochs=50,
    validation_split=0.2,
    callbacks=[early_stopping]
)

best_model.save('best_nn_model.h5')
print("Best model saved as 'best_nn_model.h5'")

test_loss, test_mae = best_model.evaluate(X_test_vectorized, target_test)
print(f"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}")

"""### CONCLUSION
The model's performance improved significantly throughout the training process, as indicated by the reduction in both training and validation loss.
Despite initial instability and high validation error in early epochs, the model achieved a Test Loss of 78.7609 and a Test MAE of 7.3042.
These results suggest that the model has learned to generalize reasonably well, though further tuning or adjustments to the architecture and hyperparameters might enhance stability and performance, particularly in the early epochs.

### EVALUATING OF NEURAL NETWORKS
"""

def evaluate_nn_model(model, X, y):

    predictions = model.predict(X).flatten()
    rmse = np.sqrt(mean_squared_error(y, predictions))
    mae = mean_absolute_error(y, predictions)
    r2 = r2_score(y, predictions)

    print(f'RMSE: {rmse:.3f}')
    print(f'MAE: {mae:.3f}')
    print(f'R²: {r2:.3f}\n')

    return {
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2
    }

nn_results = []

print('\nEvaluating Neural Network Model (Validation Set)')
val_metrics = evaluate_nn_model(best_model, X_validation_vectorized, target_validation)
nn_results.append({
    "Set": "Validation",
    "RMSE": val_metrics["RMSE"],
    "MAE": val_metrics["MAE"],
    "R2": val_metrics["R2"]
})

print('\nEvaluating Neural Network Model (Test Set)')
test_metrics = evaluate_nn_model(best_model, X_test_vectorized, target_test)
nn_results.append({
    "Set": "Test",
    "RMSE": test_metrics["RMSE"],
    "MAE": test_metrics["MAE"],
    "R2": test_metrics["R2"]
})

nn_results_df = pd.DataFrame(nn_results)
print(nn_results_df)

"""### CONCLUSION
The neural network model demonstrates moderate performance, with an R² of -0.049671 on the validation set and 0.007243 on the test set, indicating limited predictive capability.
The RMSE and MAE values suggest the model has room for improvement, particularly in minimizing prediction errors.
Further tuning of the model architecture, hyperparameters, or exploration of additional features might enhance its performance."""











